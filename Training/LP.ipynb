{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ph-Al_UtIJsE",
        "AeNbFK9-FcS5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apZAJnzSaf10",
        "outputId": "af84ec42-625a-4ee9-a1a8-5f2ce5cc0360"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOwFBOmWaolp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2c5a40-247b-43f4-fc75-d194902097c2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import jaccard_score, f1_score, hamming_loss, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "## http://scikit.ml/api/skmultilearn.adapt.mlaram.html\n",
        "!pip install scikit-multilearn\n",
        "from skmultilearn.problem_transform import LabelPowerset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-multilearn\n",
            "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 89 kB 3.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE0JNGwPautH"
      },
      "source": [
        "DF = pd.read_csv(\"/content/gdrive/MyDrive/MI_Paper/4_Final_Dataset/PMM_train.csv\")\n",
        "TEST = pd.read_csv(\"/content/gdrive/MyDrive/MI_Paper/4_Final_Dataset/PMM_test.csv\")\n",
        "\n",
        "OUTCOME_old = [\"FIBR_PREDS\",\"PREDS_TAH\",\"JELUD_TAH\",\"FIBR_JELUD\",\"A_V_BLOK\",\"OTEK_LANC\",\"RAZRIV\",\"DRESSLER\",\"ZSN\",\"REC_IM\",\"P_IM_STEN\",\"LET_IS\"]\n",
        "OUTCOME = [\"Arrhythmia\",\"MyocardialDressler\",\"CHF\",\"MI/angina\",\"LET_IS\"]\n",
        "FEATURES = list(set(list(DF.columns)).difference(set(OUTCOME)).difference(set(OUTCOME_old)))\n",
        "\n",
        "DF_X = DF.loc[:,FEATURES]\n",
        "DF_Y = DF.loc[:,OUTCOME]\n",
        "TEST_X = TEST.loc[:,FEATURES]\n",
        "TEST_Y = TEST.loc[:,OUTCOME]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gKdfx7Z4E31"
      },
      "source": [
        "def scatter_outcome(pre,new):\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(24, 5))\n",
        "  axe = axes.ravel()\n",
        "  new['Arrhythmia'].value_counts().plot(ax = axe[0], kind='bar', title='Arrhythmia')\n",
        "  pre['Arrhythmia'].value_counts().plot(ax = axe[0], marker='o', title='Arrhythmia',color='red')\n",
        "\n",
        "  new['MyocardialDressler'].value_counts().plot(ax = axe[1], kind='bar', title='Myocardial or Dressler')\n",
        "  pre['MyocardialDressler'].value_counts().plot(ax = axe[1], marker='o', title='Myocardial or Dressler',color='red')\n",
        "\n",
        "  new['CHF'].value_counts().plot(ax = axe[2], kind='bar', title='Congestive Heart Failure')\n",
        "  pre['CHF'].value_counts().plot(ax = axe[2], marker='o', title='Congestive Heart Failure',color='red')\n",
        "\n",
        "  new['MI/angina'].value_counts().plot(ax = axe[3], kind='bar', title='Post-infarction MI/angina')\n",
        "  pre['MI/angina'].value_counts().plot(ax = axe[3], marker='o', title='Post-infarction MI/angina',color='red')\n",
        "\n",
        "  new['LET_IS'].value_counts().plot(ax = axe[4], kind='bar', title='Lethal Outcome')\n",
        "  pre['LET_IS'].value_counts().plot(ax = axe[4], marker='o', title='Lethal Outcome',color='red')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-H1Y0OaDjnp"
      },
      "source": [
        "ql=[0.05, 1.]\n",
        "# Function to print the IRLbl values, measure for class imbalance\n",
        "def print_irlbl (df):\n",
        "    irlbl = df.sum(axis=0)\n",
        "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
        "    irlbl = irlbl.max() / irlbl\n",
        "    print (irlbl)\n",
        "    print(\"Mean: \", irlbl.mean())\n",
        "\n",
        "# Function to find the underrepresented targets, which are defined as those observed less than the median occurance    \n",
        "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
        "    irlbl = df.sum(axis=0)\n",
        "    #irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
        "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
        "    irlbl = irlbl.max() / irlbl\n",
        "    threshold_irlbl = irlbl.median()\n",
        "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
        "    return tail_label\n",
        "\n",
        "# Function to get the minority samples from the dataset\n",
        "# Function returns the feature vector minority dataframe as X_sub and target vector minority dataframe as y_sub\n",
        "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
        "    tail_labels = get_tail_label(y, ql=ql)\n",
        "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
        "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
        "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
        "    return X_sub, y_sub\n",
        "\n",
        "# Function that gives the index of nearest neighbor of all the instances\n",
        "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
        "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
        "    euclidean, indices = nbs.kneighbors(X)\n",
        "    return indices\n",
        "\n",
        "# Function that gets the augmented data using the MLSMOTE algorithm\n",
        "# Input: X is the input vector DataFrame, y is the feature vector dataframe, n_sample is the number of newly generated sample\n",
        "# Output: new_X is the augmented feature vector data, target is the augmented target vector data\n",
        "def MLSMOTE(X, y, n_sample, neigh=3):\n",
        "    indices2 = nearest_neighbour(X, neigh=5) # Get indices of the 5 nearest neighbors\n",
        "    n = len(indices2)\n",
        "    new_X = np.zeros((n_sample, X.shape[1]))\n",
        "    target = np.zeros((n_sample, y.shape[1]))\n",
        "    \n",
        "    # Subsets of numerical and categorical columns for different methods of generating the x features\n",
        "    Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
        "    No_Need=['ID']\n",
        "    Categorical=list(set(list(X.columns)).difference(set(Numerical)).difference(set(No_Need)))\n",
        "    \n",
        "    for i in range(n_sample):\n",
        "        reference = random.randint(0, n-1) # Randomly select 1 of the k nearest neighbors\n",
        "        neighbor = random.choice(indices2[reference, 1:])\n",
        "        all_point = indices2[reference]\n",
        "        nn_df = y[y.index.isin(all_point)]\n",
        "        ser = nn_df.sum(axis = 0, skipna = True)\n",
        "        target[i] = np.array([1 if val > 0 else 0 for val in ser])  # Create synthetic y instance\n",
        "        \n",
        "        ratio = random.random()\n",
        "        gap = X.loc[reference,Numerical] - X.loc[neighbor,Numerical]\n",
        "        X_NUM = np.array(X.loc[reference,Numerical] + ratio * gap)\n",
        "        X_CAT = np.array((X.loc[all_point,Categorical].mode()).loc[0])\n",
        "        new_X[i] = np.concatenate((X_NUM, X_CAT), axis=0)\n",
        "        \n",
        "    new_X = pd.DataFrame(new_X, columns=Numerical+Categorical)\n",
        "    target = pd.DataFrame(target, columns=y.columns)\n",
        "    return new_X, target\n",
        "\n",
        "def resample_dataset(train_X,train_Y):\n",
        "  X_sub, y_sub = get_minority_samples(train_X, train_Y)  # Getting minority samples of that dataframe\n",
        "\n",
        "  X_sub = train_X\n",
        "  y_sub = train_Y\n",
        "\n",
        "  #start_time = time.time()\n",
        "  for i in range(12):\n",
        "    X_min, y_min = get_minority_samples(X_sub, y_sub)  # Getting minority samples of that dataframe\n",
        "    X_res, y_res = MLSMOTE(X_min, y_min, 40, 5)  # Applying MLSMOTE to augment the dataframe\n",
        "    X_sub = pd.concat([X_sub,X_res])\n",
        "    y_sub = pd.concat([y_sub,y_res])\n",
        "  #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "  RES_X = X_sub\n",
        "  RES_Y = y_sub\n",
        "  \n",
        "  return(X_sub,y_sub)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UITyzcDrkZuB",
        "outputId": "098c3da7-0d0f-4ef5-a7b0-ae2a7d1bffa7"
      },
      "source": [
        "NUM_FOLD = 5\n",
        "kf = KFold(n_splits=NUM_FOLD, random_state=1, shuffle=True) # Split the dataset into k folds\n",
        "labels = DF_Y.columns[0:].tolist()\n",
        "\n",
        "folds = list(kf.split(np.arange(len(DF))))\n",
        "DF['fold'] = 0\n",
        "for i in range(NUM_FOLD):\n",
        "    DF['fold'][folds[i][1]] = i\n",
        "\n",
        "# Check how well the folds are stratified.\n",
        "print(\"fold                                         1    2    3    4    5   total\")\n",
        "print(\"==========================================================================\")\n",
        "for label in labels:\n",
        "    label_padded = label + \" \"*(40-len(label))\n",
        "    dist = \": \"\n",
        "    for i in range(NUM_FOLD):\n",
        "        dist += \"{:4d} \".format(DF[label][folds[i][1]].sum())\n",
        "    dist += \"{:4d} \".format(DF[label].sum())\n",
        "    print(label_padded + dist)\n",
        "label_padded = \"total\" + \" \"*(40-len(\"total\"))\n",
        "dist = \": \"\n",
        "for i in range(NUM_FOLD):\n",
        "    dist += \"{:4d} \".format(DF.iloc[folds[i][1]].shape[0])\n",
        "dist += \"{:4d} \".format(DF.shape[0])\n",
        "print(label_padded + dist)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fold                                         1    2    3    4    5   total\n",
            "==========================================================================\n",
            "Arrhythmia                              :   48   52   52   41   53  246 \n",
            "MyocardialDressler                      :   17   22   19   19   27  104 \n",
            "CHF                                     :   78   70   84   81   75  388 \n",
            "MI/angina                               :   38   46   42   47   42  215 \n",
            "LET_IS                                  :   47   40   31   39   53  210 \n",
            "total                                   :  272  272  272  272  272 1360 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9hrNevFkWJV",
        "outputId": "e0e2697f-632c-497c-8004-e2f00e0fa926"
      },
      "source": [
        "jac_train=[0]*NUM_FOLD\n",
        "f1w_train=[0]*NUM_FOLD\n",
        "f1m_train=[0]*NUM_FOLD\n",
        "accu_train=[0]*NUM_FOLD\n",
        "ham_train=[0]*NUM_FOLD\n",
        "\n",
        "jac_test=[0]*NUM_FOLD\n",
        "f1w_test=[0]*NUM_FOLD\n",
        "f1m_test=[0]*NUM_FOLD\n",
        "accu_test=[0]*NUM_FOLD\n",
        "ham_test=[0]*NUM_FOLD\n",
        "\n",
        "# Params for SVC\n",
        "#par_C = [50, 80, 100, 200, 300] \n",
        "#par_gamma=[0.001, 0.01, 0.1]\n",
        "\n",
        "# Params for logistic regression\n",
        "par_pen = ['l1'] #'l1', 'l2'\n",
        "par_C = [0.01] #0.001, 0.01, 0.1, 1, 10, 100\n",
        "\n",
        "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
        "\n",
        "for i in range(len(par_C)):\n",
        "  for j in range(len(par_pen)):\n",
        "    for fold, (train_index, test_index) in enumerate(kf.split(DF), 1):\n",
        "      X_train = DF_X.iloc[train_index]\n",
        "      Y_train = DF_Y.iloc[train_index]  \n",
        "      X_test = DF_X.iloc[test_index]\n",
        "      Y_test = DF_Y.iloc[test_index]\n",
        "\n",
        "      X_train_SCALED = X_train.copy()\n",
        "      X_testSCALED = X_test.copy()\n",
        "\n",
        "      transformer = StandardScaler().fit(X_train_SCALED[Numerical])\n",
        "      X_train_SCALED[Numerical] = transformer.transform(X_train_SCALED[Numerical])\n",
        "      X_testSCALED[Numerical] = transformer.transform(X_testSCALED[Numerical])\n",
        "    \n",
        "      ######### Resample the dataset with MLSMOTE #########\n",
        "      X_train_oversampled, Y_train_oversampled = resample_dataset(X_train_SCALED, Y_train)\n",
        "\n",
        "      ######### Train the model (change the code here for another type of model)#########\n",
        "      model = LabelPowerset(classifier = LogisticRegression(penalty = par_pen[j], C = par_C[i], solver = 'liblinear', max_iter=7000))\n",
        "      #model = LabelPowerset(classifier = SVC(C=par_C[i], gamma=par_gamma[j], kernel='rbf'))\n",
        "                        #GaussianNB()   \n",
        "                        #LogisticRegression(penalty = 'l2', C = 100, solver = 'liblinear', max_iter=7000))\n",
        "                               \n",
        "      #model = LabelPowerset(classifier = GaussianNB())\n",
        "      model.fit(X_train_oversampled, Y_train_oversampled)\n",
        "\n",
        "      y_hat = model.predict(X_train_oversampled)\n",
        "      Y_pred = model.predict(X_testSCALED)\n",
        "\n",
        "      jac_train[fold-1] = jaccard_score(Y_train_oversampled, y_hat, average='weighted')\n",
        "      f1w_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='weighted')\n",
        "      f1m_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='micro')\n",
        "      accu_train[fold-1] = accuracy_score(Y_train_oversampled, y_hat)\n",
        "      ham_train[fold-1] = hamming_loss(Y_train_oversampled, y_hat) \n",
        "    \n",
        "      jac_test[fold-1] = jaccard_score(Y_test, Y_pred, average='weighted')\n",
        "      f1w_test[fold-1] = f1_score(Y_test, Y_pred, average='weighted')\n",
        "      f1m_test[fold-1] = f1_score(Y_test, Y_pred, average='micro')\n",
        "      accu_test[fold-1] = accuracy_score(Y_test, Y_pred)\n",
        "      ham_test[fold-1] = hamming_loss(Y_test, Y_pred)\n",
        "\n",
        "\n",
        "    ######### Print the overall performance accross all folds (test set) #########\n",
        "    print(\"============================================================================\")\n",
        "    print(f'For C={par_C[i]} and pen={par_pen[j]}:')\n",
        "    print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
        "    # Training set: print result for overleaf\n",
        "    print(\"Training: \",\"& \",round(sum(f1w_train)/len(f1w_train),3),\"& \", round(sum(f1m_train)/len(f1m_train),3), \n",
        "      \"& \",round(sum(jac_train)/len(jac_train), 3),\"& \",round(sum(accu_train)/len(accu_train), 3),\n",
        "      \"& \",round(sum(ham_train)/len(ham_train),3))\n",
        "    \n",
        "    # Validation set: print result for overleaf\n",
        "    print(\"Validation: \",\"& \",round(sum(f1w_test)/len(f1w_test),3),\"& \", round(sum(f1m_test)/len(f1m_test),3), \n",
        "      \"& \",round(sum(jac_test)/len(jac_test), 3),\"& \",round(sum(accu_test)/len(accu_test), 3),\n",
        "      \"& \",round(sum(ham_test)/len(ham_test),3))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n",
            "For C=0.01 and pen=l1:\n",
            "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
            "Training:  &  0.467 &  0.467 &  0.306 &  0.324 &  0.37\n",
            "Validation:  &  0.29 &  0.28 &  0.172 &  0.332 &  0.305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRtNH3l3kWGC",
        "outputId": "668c12bd-d301-4f95-c95e-f1f2f1767ffb"
      },
      "source": [
        "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
        "DF_X_SCALED = DF_X.copy()\n",
        "TEST_XSCALED = TEST_X.copy()\n",
        "transformer = StandardScaler().fit(DF_X_SCALED[Numerical])\n",
        "DF_X_SCALED[Numerical] = transformer.transform(DF_X_SCALED[Numerical])\n",
        "TEST_XSCALED[Numerical] = transformer.transform(TEST_XSCALED[Numerical])\n",
        "\n",
        "######### Train the model (change the code here for another type of model)#########\n",
        "#model = LabelPowerset(classifier = GaussianNB())\n",
        "#model = LabelPowerset(classifier = SVC(C=200, gamma=0.001,kernel='rbf')) # SVC(C=0.1, kernel='rbf', gamma=1)\n",
        "model = LabelPowerset(classifier = LogisticRegression(penalty = 'l1', C = 100, solver = 'liblinear', max_iter=7000))\n",
        "model.fit(DF_X_SCALED, DF_Y)\n",
        "test_pred = model.predict(TEST_XSCALED)\n",
        "\n",
        "# Testing set: print result for overleaf\n",
        "print(\"============================================================================\")\n",
        "print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
        "print(\"Testing: \",\"& \",round(f1_score(TEST_Y, test_pred, average='weighted'), 3),\"& \", round(f1_score(TEST_Y, test_pred, average='micro'),3), \n",
        "  \"& \",round(jaccard_score(TEST_Y, test_pred, average='weighted'), 3),\"& \",round(accuracy_score(TEST_Y, test_pred), 3),\n",
        "  \"& \",round(hamming_loss(TEST_Y, test_pred),3))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n",
            "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
            "Testing:  &  0.322 &  0.327 &  0.201 &  0.341 &  0.232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ppXzHu_VSuRd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}