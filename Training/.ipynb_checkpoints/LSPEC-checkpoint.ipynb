{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOwFBOmWaolp",
    "outputId": "9f71ccc8-850d-436c-d0ff-6ad9d6c3f401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import jaccard_score, f1_score, hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "!pip install scikit-multilearn\n",
    "from skmultilearn.ensemble import LabelSpacePartitioningClassifier\n",
    "from skmultilearn.cluster import FixedLabelSpaceClusterer, NetworkXLabelGraphClusterer, LabelCooccurrenceGraphBuilder\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DE0JNGwPautH"
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"PMM_train.csv\")\n",
    "TEST = pd.read_csv(\"PMM_test.csv\")\n",
    "\n",
    "OUTCOME_old = [\"FIBR_PREDS\",\"PREDS_TAH\",\"JELUD_TAH\",\"FIBR_JELUD\",\"A_V_BLOK\",\"OTEK_LANC\",\"RAZRIV\",\"DRESSLER\",\"ZSN\",\"REC_IM\",\"P_IM_STEN\",\"LET_IS\"]\n",
    "OUTCOME = [\"Arrhythmia\",\"MyocardialDressler\",\"CHF\",\"MI/angina\",\"LET_IS\"]\n",
    "FEATURES = list(set(list(DF.columns)).difference(set(OUTCOME)).difference(set(OUTCOME_old)))\n",
    "\n",
    "DF_X = DF.loc[:,FEATURES]\n",
    "DF_Y = DF.loc[:,OUTCOME]\n",
    "TEST_X = TEST.loc[:,FEATURES]\n",
    "TEST_Y = TEST.loc[:,OUTCOME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5gKdfx7Z4E31"
   },
   "outputs": [],
   "source": [
    "def scatter_outcome(pre,new):\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(24, 5))\n",
    "  axe = axes.ravel()\n",
    "  new['Arrhythmia'].value_counts().plot(ax = axe[0], kind='bar', title='Arrhythmia')\n",
    "  pre['Arrhythmia'].value_counts().plot(ax = axe[0], marker='o', title='Arrhythmia',color='red')\n",
    "\n",
    "  new['MyocardialDressler'].value_counts().plot(ax = axe[1], kind='bar', title='Myocardial or Dressler')\n",
    "  pre['MyocardialDressler'].value_counts().plot(ax = axe[1], marker='o', title='Myocardial or Dressler',color='red')\n",
    "\n",
    "  new['CHF'].value_counts().plot(ax = axe[2], kind='bar', title='Congestive Heart Failure')\n",
    "  pre['CHF'].value_counts().plot(ax = axe[2], marker='o', title='Congestive Heart Failure',color='red')\n",
    "\n",
    "  new['MI/angina'].value_counts().plot(ax = axe[3], kind='bar', title='Post-infarction MI/angina')\n",
    "  pre['MI/angina'].value_counts().plot(ax = axe[3], marker='o', title='Post-infarction MI/angina',color='red')\n",
    "\n",
    "  new['LET_IS'].value_counts().plot(ax = axe[4], kind='bar', title='Lethal Outcome')\n",
    "  pre['LET_IS'].value_counts().plot(ax = axe[4], marker='o', title='Lethal Outcome',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a-H1Y0OaDjnp"
   },
   "outputs": [],
   "source": [
    "ql=[0.05, 1.]\n",
    "# Function to print the IRLbl values, measure for class imbalance\n",
    "def print_irlbl (df):\n",
    "    irlbl = df.sum(axis=0)\n",
    "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    print (irlbl)\n",
    "    print(\"Mean: \", irlbl.mean())\n",
    "\n",
    "# Function to find the underrepresented targets, which are defined as those observed less than the median occurance    \n",
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    irlbl = df.sum(axis=0)\n",
    "    #irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_label\n",
    "\n",
    "# Function to get the minority samples from the dataset\n",
    "# Function returns the feature vector minority dataframe as X_sub and target vector minority dataframe as y_sub\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "# Function that gives the index of nearest neighbor of all the instances\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "# Function that gets the augmented data using the MLSMOTE algorithm\n",
    "# Input: X is the input vector DataFrame, y is the feature vector dataframe, n_sample is the number of newly generated sample\n",
    "# Output: new_X is the augmented feature vector data, target is the augmented target vector data\n",
    "def MLSMOTE(X, y, n_sample, neigh=3):\n",
    "    indices2 = nearest_neighbour(X, neigh=5) # Get indices of the 5 nearest neighbors\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    \n",
    "    # Subsets of numerical and categorical columns for different methods of generating the x features\n",
    "    Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "    No_Need=['ID']\n",
    "    Categorical=list(set(list(X.columns)).difference(set(Numerical)).difference(set(No_Need)))\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n-1) # Randomly select 1 of the k nearest neighbors\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])  # Create synthetic y instance\n",
    "        \n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,Numerical] - X.loc[neighbor,Numerical]\n",
    "        X_NUM = np.array(X.loc[reference,Numerical] + ratio * gap)\n",
    "        X_CAT = np.array((X.loc[all_point,Categorical].mode()).loc[0])\n",
    "        new_X[i] = np.concatenate((X_NUM, X_CAT), axis=0)\n",
    "        \n",
    "    new_X = pd.DataFrame(new_X, columns=Numerical+Categorical)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target\n",
    "\n",
    "def resample_dataset(train_X,train_Y):\n",
    "  X_sub, y_sub = get_minority_samples(train_X, train_Y)  # Getting minority samples of that dataframe\n",
    "\n",
    "  X_sub = train_X\n",
    "  y_sub = train_Y\n",
    "\n",
    "  #start_time = time.time()\n",
    "  for i in range(12):\n",
    "    X_min, y_min = get_minority_samples(X_sub, y_sub)  # Getting minority samples of that dataframe\n",
    "    X_res, y_res = MLSMOTE(X_min, y_min, 40, 5)  # Applying MLSMOTE to augment the dataframe\n",
    "    X_sub = pd.concat([X_sub,X_res])\n",
    "    y_sub = pd.concat([y_sub,y_res])\n",
    "  #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "  RES_X = X_sub\n",
    "  RES_Y = y_sub\n",
    "  \n",
    "  return(X_sub,y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UITyzcDrkZuB",
    "outputId": "f892da4f-9be7-49c2-b4f8-7b59361fc425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold                                         1    2    3    4    5   total\n",
      "==========================================================================\n",
      "Arrhythmia                              :   48   52   52   41   53  246 \n",
      "MyocardialDressler                      :   17   22   19   19   27  104 \n",
      "CHF                                     :   78   70   84   81   75  388 \n",
      "MI/angina                               :   38   46   42   47   42  215 \n",
      "LET_IS                                  :   47   40   31   39   53  210 \n",
      "total                                   :  272  272  272  272  272 1360 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "NUM_FOLD = 5\n",
    "kf = KFold(n_splits=NUM_FOLD, random_state=1, shuffle=True) # Split the dataset into k folds\n",
    "labels = DF_Y.columns[0:].tolist()\n",
    "\n",
    "folds = list(kf.split(np.arange(len(DF))))\n",
    "DF['fold'] = 0\n",
    "for i in range(NUM_FOLD):\n",
    "    DF['fold'][folds[i][1]] = i\n",
    "\n",
    "# Check how well the folds are stratified.\n",
    "print(\"fold                                         1    2    3    4    5   total\")\n",
    "print(\"==========================================================================\")\n",
    "for label in labels:\n",
    "    label_padded = label + \" \"*(40-len(label))\n",
    "    dist = \": \"\n",
    "    for i in range(NUM_FOLD):\n",
    "        dist += \"{:4d} \".format(DF[label][folds[i][1]].sum())\n",
    "    dist += \"{:4d} \".format(DF[label].sum())\n",
    "    print(label_padded + dist)\n",
    "label_padded = \"total\" + \" \"*(40-len(\"total\"))\n",
    "dist = \": \"\n",
    "for i in range(NUM_FOLD):\n",
    "    dist += \"{:4d} \".format(DF.iloc[folds[i][1]].shape[0])\n",
    "dist += \"{:4d} \".format(DF.shape[0])\n",
    "print(label_padded + dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9hrNevFkWJV",
    "outputId": "2bddfd12-5e43-485c-af50-8e2bd777b1e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "For C=80 and gamma=0.01:\n",
      "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
      "Training:  &  0.98 &  0.98 &  0.961 &  0.937 &  0.016\n",
      "Validation:  &  0.318 &  0.314 &  0.194 &  0.3 &  0.252\n"
     ]
    }
   ],
   "source": [
    "jac_train=[0]*NUM_FOLD\n",
    "f1w_train=[0]*NUM_FOLD\n",
    "f1m_train=[0]*NUM_FOLD\n",
    "accu_train=[0]*NUM_FOLD\n",
    "ham_train=[0]*NUM_FOLD\n",
    "\n",
    "jac_test=[0]*NUM_FOLD\n",
    "f1w_test=[0]*NUM_FOLD\n",
    "f1m_test=[0]*NUM_FOLD\n",
    "accu_test=[0]*NUM_FOLD\n",
    "ham_test=[0]*NUM_FOLD\n",
    "\n",
    "# Params for SVC\n",
    "par_C = [80]  #50, 80, 100, 200, 300\n",
    "par_gamma=[0.01] #0.001, 0.01, 0.1\n",
    "\n",
    "# Params for logistic regression\n",
    "#par_pen = ['l1', 'l2']\n",
    "#par_C = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "\n",
    "for i in range(len(par_C)):\n",
    "  for j in range(len(par_gamma)):\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(DF), 1):\n",
    "      X_train = DF_X.iloc[train_index]\n",
    "      Y_train = DF_Y.iloc[train_index]  \n",
    "      X_test = DF_X.iloc[test_index]\n",
    "      Y_test = DF_Y.iloc[test_index]\n",
    "\n",
    "      X_train_SCALED = X_train.copy()\n",
    "      X_testSCALED = X_test.copy()\n",
    "\n",
    "      transformer = StandardScaler().fit(X_train_SCALED[Numerical])\n",
    "      X_train_SCALED[Numerical] = transformer.transform(X_train_SCALED[Numerical])\n",
    "      X_testSCALED[Numerical] = transformer.transform(X_testSCALED[Numerical])\n",
    "    \n",
    "      ######### Resample the dataset with MLSMOTE #########\n",
    "      X_train_oversampled, Y_train_oversampled = resample_dataset(X_train_SCALED, Y_train)\n",
    "\n",
    "      ######### Train the model (change the code here for another type of model)#########\n",
    "      model = LabelSpacePartitioningClassifier(classifier = BinaryRelevance(classifier=SVC(C=par_C[i], gamma=par_gamma[j], kernel='rbf')), #ClassifierChain(SVC(C=par_C[i], gamma=par_gamma[j])), GaussianNB(), LogisticRegression(penalty = par_pen[j], C = par_C[i], solver = 'liblinear', max_iter=7000)\n",
    "                           clusterer = FixedLabelSpaceClusterer(clusters = [[0,1,2],[0,1,3],[0,1,4],[0,2,3],[0,2,4],[0,3,4],[1,2,3],[1,2,4],[1,3,4],[2,3,4]]))\n",
    "                           #clusterer = NetworkXLabelGraphClusterer(graph_builder, method='louvain')) #\n",
    "      model.fit(csr_matrix(np.array(X_train_oversampled)).toarray(), csr_matrix(np.array(Y_train_oversampled)).toarray())\n",
    "\n",
    "      y_hat = model.predict(X_train_oversampled)\n",
    "      Y_pred = model.predict(X_testSCALED)\n",
    "\n",
    "      jac_train[fold-1] = jaccard_score(Y_train_oversampled, y_hat, average='weighted')\n",
    "      f1w_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='weighted')\n",
    "      f1m_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='micro')\n",
    "      accu_train[fold-1] = accuracy_score(Y_train_oversampled, y_hat)\n",
    "      ham_train[fold-1] = hamming_loss(Y_train_oversampled, y_hat) \n",
    "    \n",
    "      jac_test[fold-1] = jaccard_score(Y_test, Y_pred, average='weighted')\n",
    "      f1w_test[fold-1] = f1_score(Y_test, Y_pred, average='weighted')\n",
    "      f1m_test[fold-1] = f1_score(Y_test, Y_pred, average='micro')\n",
    "      accu_test[fold-1] = accuracy_score(Y_test, Y_pred)\n",
    "      ham_test[fold-1] = hamming_loss(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "    ######### Print the overall performance accross all folds (test set) #########\n",
    "    print(\"============================================================================\")\n",
    "    print(f'For C={par_C[i]} and gamma={par_gamma[j]}:')\n",
    "    print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
    "    # Training set: print result for overleaf\n",
    "    print(\"Training: \",\"& \",round(sum(f1w_train)/len(f1w_train),3),\"& \", round(sum(f1m_train)/len(f1m_train),3), \n",
    "      \"& \",round(sum(jac_train)/len(jac_train), 3),\"& \",round(sum(accu_train)/len(accu_train), 3),\n",
    "      \"& \",round(sum(ham_train)/len(ham_train),3))\n",
    "    \n",
    "    # Validation set: print result for overleaf\n",
    "    print(\"Validation: \",\"& \",round(sum(f1w_test)/len(f1w_test),3),\"& \", round(sum(f1m_test)/len(f1m_test),3), \n",
    "      \"& \",round(sum(jac_test)/len(jac_test), 3),\"& \",round(sum(accu_test)/len(accu_test), 3),\n",
    "      \"& \",round(sum(ham_test)/len(ham_test),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRtNH3l3kWGC",
    "outputId": "bba8bdbf-3c5d-4dc8-d2f5-298c02a296c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
      "Testing:  &  0.383 &  0.328 &  0.24 &  0.003 &  0.635\n"
     ]
    }
   ],
   "source": [
    "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "DF_X_SCALED = DF_X.copy()\n",
    "TEST_XSCALED = TEST_X.copy()\n",
    "transformer = StandardScaler().fit(DF_X_SCALED[Numerical])\n",
    "DF_X_SCALED[Numerical] = transformer.transform(DF_X_SCALED[Numerical])\n",
    "TEST_XSCALED[Numerical] = transformer.transform(TEST_XSCALED[Numerical])\n",
    "\n",
    "######### Train the model (change the code here for another type of model)#########\n",
    "model = LabelSpacePartitioningClassifier(classifier = BinaryRelevance(SVC(C=80, gamma=0.01, kernel='rbf')), #SVC(C=80, gamma=0.01),GaussianNB(), LogisticRegression(penalty = 'l2', C = 1, solver = 'liblinear', max_iter=7000)\n",
    "                      clusterer = FixedLabelSpaceClusterer(clusters = [[0,1,2],[0,1,3],[0,1,4],[0,2,3],[0,2,4],[0,3,4],[1,2,3],[1,2,4],[1,3,4],[2,3,4]]))\n",
    "\n",
    "model.fit(DF_X_SCALED, DF_Y)\n",
    "test_pred = model.predict(TEST_XSCALED)\n",
    "# Testing set: print result for overleaf\n",
    "print(\"============================================================================\")\n",
    "print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
    "print(\"Testing: \",\"& \",round(f1_score(TEST_Y, test_pred, average='weighted'), 3),\"& \", round(f1_score(TEST_Y, test_pred, average='micro'),3), \n",
    "  \"& \",round(jaccard_score(TEST_Y, test_pred, average='weighted'), 3),\"& \",round(accuracy_score(TEST_Y, test_pred), 3),\n",
    "  \"& \",round(hamming_loss(TEST_Y, test_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwxV7HRWWZPA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ph-Al_UtIJsE",
    "AeNbFK9-FcS5"
   ],
   "name": "LSPEC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
