{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOwFBOmWaolp",
    "outputId": "ee2c5a40-247b-43f4-fc75-d194902097c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-multilearn\n",
      "  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 3.6 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import jaccard_score, f1_score, hamming_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "## http://scikit.ml/api/skmultilearn.adapt.mlaram.html\n",
    "!pip install scikit-multilearn\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DE0JNGwPautH"
   },
   "outputs": [],
   "source": [
    "DF = pd.read_csv(\"PMM_train.csv\")\n",
    "TEST = pd.read_csv(\"PMM_test.csv\")\n",
    "\n",
    "OUTCOME_old = [\"FIBR_PREDS\",\"PREDS_TAH\",\"JELUD_TAH\",\"FIBR_JELUD\",\"A_V_BLOK\",\"OTEK_LANC\",\"RAZRIV\",\"DRESSLER\",\"ZSN\",\"REC_IM\",\"P_IM_STEN\",\"LET_IS\"]\n",
    "OUTCOME = [\"Arrhythmia\",\"MyocardialDressler\",\"CHF\",\"MI/angina\",\"LET_IS\"]\n",
    "FEATURES = list(set(list(DF.columns)).difference(set(OUTCOME)).difference(set(OUTCOME_old)))\n",
    "\n",
    "DF_X = DF.loc[:,FEATURES]\n",
    "DF_Y = DF.loc[:,OUTCOME]\n",
    "TEST_X = TEST.loc[:,FEATURES]\n",
    "TEST_Y = TEST.loc[:,OUTCOME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5gKdfx7Z4E31"
   },
   "outputs": [],
   "source": [
    "def scatter_outcome(pre,new):\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(24, 5))\n",
    "  axe = axes.ravel()\n",
    "  new['Arrhythmia'].value_counts().plot(ax = axe[0], kind='bar', title='Arrhythmia')\n",
    "  pre['Arrhythmia'].value_counts().plot(ax = axe[0], marker='o', title='Arrhythmia',color='red')\n",
    "\n",
    "  new['MyocardialDressler'].value_counts().plot(ax = axe[1], kind='bar', title='Myocardial or Dressler')\n",
    "  pre['MyocardialDressler'].value_counts().plot(ax = axe[1], marker='o', title='Myocardial or Dressler',color='red')\n",
    "\n",
    "  new['CHF'].value_counts().plot(ax = axe[2], kind='bar', title='Congestive Heart Failure')\n",
    "  pre['CHF'].value_counts().plot(ax = axe[2], marker='o', title='Congestive Heart Failure',color='red')\n",
    "\n",
    "  new['MI/angina'].value_counts().plot(ax = axe[3], kind='bar', title='Post-infarction MI/angina')\n",
    "  pre['MI/angina'].value_counts().plot(ax = axe[3], marker='o', title='Post-infarction MI/angina',color='red')\n",
    "\n",
    "  new['LET_IS'].value_counts().plot(ax = axe[4], kind='bar', title='Lethal Outcome')\n",
    "  pre['LET_IS'].value_counts().plot(ax = axe[4], marker='o', title='Lethal Outcome',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a-H1Y0OaDjnp"
   },
   "outputs": [],
   "source": [
    "ql=[0.05, 1.]\n",
    "# Function to print the IRLbl values, measure for class imbalance\n",
    "def print_irlbl (df):\n",
    "    irlbl = df.sum(axis=0)\n",
    "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    print (irlbl)\n",
    "    print(\"Mean: \", irlbl.mean())\n",
    "\n",
    "# Function to find the underrepresented targets, which are defined as those observed less than the median occurance    \n",
    "def get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n",
    "    irlbl = df.sum(axis=0)\n",
    "    #irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    #irlbl = irlbl[(irlbl < irlbl.quantile(ql[1]))]\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_label\n",
    "\n",
    "# Function to get the minority samples from the dataset\n",
    "# Function returns the feature vector minority dataframe as X_sub and target vector minority dataframe as y_sub\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n",
    "    tail_labels = get_tail_label(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "# Function that gives the index of nearest neighbor of all the instances\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "# Function that gets the augmented data using the MLSMOTE algorithm\n",
    "# Input: X is the input vector DataFrame, y is the feature vector dataframe, n_sample is the number of newly generated sample\n",
    "# Output: new_X is the augmented feature vector data, target is the augmented target vector data\n",
    "def MLSMOTE(X, y, n_sample, neigh=3):\n",
    "    indices2 = nearest_neighbour(X, neigh=5) # Get indices of the 5 nearest neighbors\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_sample, X.shape[1]))\n",
    "    target = np.zeros((n_sample, y.shape[1]))\n",
    "    \n",
    "    # Subsets of numerical and categorical columns for different methods of generating the x features\n",
    "    Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "    No_Need=['ID']\n",
    "    Categorical=list(set(list(X.columns)).difference(set(Numerical)).difference(set(No_Need)))\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        reference = random.randint(0, n-1) # Randomly select 1 of the k nearest neighbors\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])  # Create synthetic y instance\n",
    "        \n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,Numerical] - X.loc[neighbor,Numerical]\n",
    "        X_NUM = np.array(X.loc[reference,Numerical] + ratio * gap)\n",
    "        X_CAT = np.array((X.loc[all_point,Categorical].mode()).loc[0])\n",
    "        new_X[i] = np.concatenate((X_NUM, X_CAT), axis=0)\n",
    "        \n",
    "    new_X = pd.DataFrame(new_X, columns=Numerical+Categorical)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target\n",
    "\n",
    "def resample_dataset(train_X,train_Y):\n",
    "  X_sub, y_sub = get_minority_samples(train_X, train_Y)  # Getting minority samples of that dataframe\n",
    "\n",
    "  X_sub = train_X\n",
    "  y_sub = train_Y\n",
    "\n",
    "  #start_time = time.time()\n",
    "  for i in range(12):\n",
    "    X_min, y_min = get_minority_samples(X_sub, y_sub)  # Getting minority samples of that dataframe\n",
    "    X_res, y_res = MLSMOTE(X_min, y_min, 40, 5)  # Applying MLSMOTE to augment the dataframe\n",
    "    X_sub = pd.concat([X_sub,X_res])\n",
    "    y_sub = pd.concat([y_sub,y_res])\n",
    "  #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "  RES_X = X_sub\n",
    "  RES_Y = y_sub\n",
    "  \n",
    "  return(X_sub,y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UITyzcDrkZuB",
    "outputId": "098c3da7-0d0f-4ef5-a7b0-ae2a7d1bffa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold                                         1    2    3    4    5   total\n",
      "==========================================================================\n",
      "Arrhythmia                              :   48   52   52   41   53  246 \n",
      "MyocardialDressler                      :   17   22   19   19   27  104 \n",
      "CHF                                     :   78   70   84   81   75  388 \n",
      "MI/angina                               :   38   46   42   47   42  215 \n",
      "LET_IS                                  :   47   40   31   39   53  210 \n",
      "total                                   :  272  272  272  272  272 1360 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "NUM_FOLD = 5\n",
    "kf = KFold(n_splits=NUM_FOLD, random_state=1, shuffle=True) # Split the dataset into k folds\n",
    "labels = DF_Y.columns[0:].tolist()\n",
    "\n",
    "folds = list(kf.split(np.arange(len(DF))))\n",
    "DF['fold'] = 0\n",
    "for i in range(NUM_FOLD):\n",
    "    DF['fold'][folds[i][1]] = i\n",
    "\n",
    "# Check how well the folds are stratified.\n",
    "print(\"fold                                         1    2    3    4    5   total\")\n",
    "print(\"==========================================================================\")\n",
    "for label in labels:\n",
    "    label_padded = label + \" \"*(40-len(label))\n",
    "    dist = \": \"\n",
    "    for i in range(NUM_FOLD):\n",
    "        dist += \"{:4d} \".format(DF[label][folds[i][1]].sum())\n",
    "    dist += \"{:4d} \".format(DF[label].sum())\n",
    "    print(label_padded + dist)\n",
    "label_padded = \"total\" + \" \"*(40-len(\"total\"))\n",
    "dist = \": \"\n",
    "for i in range(NUM_FOLD):\n",
    "    dist += \"{:4d} \".format(DF.iloc[folds[i][1]].shape[0])\n",
    "dist += \"{:4d} \".format(DF.shape[0])\n",
    "print(label_padded + dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9hrNevFkWJV",
    "outputId": "e0e2697f-632c-497c-8004-e2f00e0fa926"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "For C=0.01 and pen=l1:\n",
      "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
      "Training:  &  0.467 &  0.467 &  0.306 &  0.324 &  0.37\n",
      "Validation:  &  0.29 &  0.28 &  0.172 &  0.332 &  0.305\n"
     ]
    }
   ],
   "source": [
    "jac_train=[0]*NUM_FOLD\n",
    "f1w_train=[0]*NUM_FOLD\n",
    "f1m_train=[0]*NUM_FOLD\n",
    "accu_train=[0]*NUM_FOLD\n",
    "ham_train=[0]*NUM_FOLD\n",
    "\n",
    "jac_test=[0]*NUM_FOLD\n",
    "f1w_test=[0]*NUM_FOLD\n",
    "f1m_test=[0]*NUM_FOLD\n",
    "accu_test=[0]*NUM_FOLD\n",
    "ham_test=[0]*NUM_FOLD\n",
    "\n",
    "# Params for SVC\n",
    "#par_C = [50, 80, 100, 200, 300] \n",
    "#par_gamma=[0.001, 0.01, 0.1]\n",
    "\n",
    "# Params for logistic regression\n",
    "par_pen = ['l1'] #'l1', 'l2'\n",
    "par_C = [0.01] #0.001, 0.01, 0.1, 1, 10, 100\n",
    "\n",
    "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "\n",
    "for i in range(len(par_C)):\n",
    "  for j in range(len(par_pen)):\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(DF), 1):\n",
    "      X_train = DF_X.iloc[train_index]\n",
    "      Y_train = DF_Y.iloc[train_index]  \n",
    "      X_test = DF_X.iloc[test_index]\n",
    "      Y_test = DF_Y.iloc[test_index]\n",
    "\n",
    "      X_train_SCALED = X_train.copy()\n",
    "      X_testSCALED = X_test.copy()\n",
    "\n",
    "      transformer = StandardScaler().fit(X_train_SCALED[Numerical])\n",
    "      X_train_SCALED[Numerical] = transformer.transform(X_train_SCALED[Numerical])\n",
    "      X_testSCALED[Numerical] = transformer.transform(X_testSCALED[Numerical])\n",
    "    \n",
    "      ######### Resample the dataset with MLSMOTE #########\n",
    "      X_train_oversampled, Y_train_oversampled = resample_dataset(X_train_SCALED, Y_train)\n",
    "\n",
    "      ######### Train the model (change the code here for another type of model)#########\n",
    "      model = LabelPowerset(classifier = LogisticRegression(penalty = par_pen[j], C = par_C[i], solver = 'liblinear', max_iter=7000))\n",
    "      #model = LabelPowerset(classifier = SVC(C=par_C[i], gamma=par_gamma[j], kernel='rbf'))\n",
    "                        #GaussianNB()   \n",
    "                        #LogisticRegression(penalty = 'l2', C = 100, solver = 'liblinear', max_iter=7000))\n",
    "                               \n",
    "      #model = LabelPowerset(classifier = GaussianNB())\n",
    "      model.fit(X_train_oversampled, Y_train_oversampled)\n",
    "\n",
    "      y_hat = model.predict(X_train_oversampled)\n",
    "      Y_pred = model.predict(X_testSCALED)\n",
    "\n",
    "      jac_train[fold-1] = jaccard_score(Y_train_oversampled, y_hat, average='weighted')\n",
    "      f1w_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='weighted')\n",
    "      f1m_train[fold-1] = f1_score(Y_train_oversampled, y_hat, average='micro')\n",
    "      accu_train[fold-1] = accuracy_score(Y_train_oversampled, y_hat)\n",
    "      ham_train[fold-1] = hamming_loss(Y_train_oversampled, y_hat) \n",
    "    \n",
    "      jac_test[fold-1] = jaccard_score(Y_test, Y_pred, average='weighted')\n",
    "      f1w_test[fold-1] = f1_score(Y_test, Y_pred, average='weighted')\n",
    "      f1m_test[fold-1] = f1_score(Y_test, Y_pred, average='micro')\n",
    "      accu_test[fold-1] = accuracy_score(Y_test, Y_pred)\n",
    "      ham_test[fold-1] = hamming_loss(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "    ######### Print the overall performance accross all folds (test set) #########\n",
    "    print(\"============================================================================\")\n",
    "    print(f'For C={par_C[i]} and pen={par_pen[j]}:')\n",
    "    print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
    "    # Training set: print result for overleaf\n",
    "    print(\"Training: \",\"& \",round(sum(f1w_train)/len(f1w_train),3),\"& \", round(sum(f1m_train)/len(f1m_train),3), \n",
    "      \"& \",round(sum(jac_train)/len(jac_train), 3),\"& \",round(sum(accu_train)/len(accu_train), 3),\n",
    "      \"& \",round(sum(ham_train)/len(ham_train),3))\n",
    "    \n",
    "    # Validation set: print result for overleaf\n",
    "    print(\"Validation: \",\"& \",round(sum(f1w_test)/len(f1w_test),3),\"& \", round(sum(f1m_test)/len(f1m_test),3), \n",
    "      \"& \",round(sum(jac_test)/len(jac_test), 3),\"& \",round(sum(accu_test)/len(accu_test), 3),\n",
    "      \"& \",round(sum(ham_test)/len(ham_test),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRtNH3l3kWGC",
    "outputId": "668c12bd-d301-4f95-c95e-f1f2f1767ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================\n",
      "          F1-w  F1-mi  Jaccard  Accuracy  Hamming\n",
      "Testing:  &  0.322 &  0.327 &  0.201 &  0.341 &  0.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "Numerical=['AGE','S_AD_ORIT','D_AD_ORIT','K_BLOOD','NA_BLOOD','ALT_BLOOD','AST_BLOOD','L_BLOOD','ROE']\n",
    "DF_X_SCALED = DF_X.copy()\n",
    "TEST_XSCALED = TEST_X.copy()\n",
    "transformer = StandardScaler().fit(DF_X_SCALED[Numerical])\n",
    "DF_X_SCALED[Numerical] = transformer.transform(DF_X_SCALED[Numerical])\n",
    "TEST_XSCALED[Numerical] = transformer.transform(TEST_XSCALED[Numerical])\n",
    "\n",
    "######### Train the model (change the code here for another type of model)#########\n",
    "#model = LabelPowerset(classifier = GaussianNB())\n",
    "#model = LabelPowerset(classifier = SVC(C=200, gamma=0.001,kernel='rbf')) # SVC(C=0.1, kernel='rbf', gamma=1)\n",
    "model = LabelPowerset(classifier = LogisticRegression(penalty = 'l1', C = 100, solver = 'liblinear', max_iter=7000))\n",
    "model.fit(DF_X_SCALED, DF_Y)\n",
    "test_pred = model.predict(TEST_XSCALED)\n",
    "\n",
    "# Testing set: print result for overleaf\n",
    "print(\"============================================================================\")\n",
    "print(\"          F1-w \", \"F1-mi \", \"Jaccard \", \"Accuracy \", \"Hamming\")\n",
    "print(\"Testing: \",\"& \",round(f1_score(TEST_Y, test_pred, average='weighted'), 3),\"& \", round(f1_score(TEST_Y, test_pred, average='micro'),3), \n",
    "  \"& \",round(jaccard_score(TEST_Y, test_pred, average='weighted'), 3),\"& \",round(accuracy_score(TEST_Y, test_pred), 3),\n",
    "  \"& \",round(hamming_loss(TEST_Y, test_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppXzHu_VSuRd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ph-Al_UtIJsE",
    "AeNbFK9-FcS5"
   ],
   "name": "LP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
